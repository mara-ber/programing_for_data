{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "socyySfaK8_B"
   },
   "source": [
    "# Data retrieval\n",
    "---\n",
    "\n",
    "Examples of data being retrieved from a range of sources, with exercises.\n",
    "\n",
    "This worksheet (and its successors) use the python **pandas library** to retrieve data from a variety of data file types, online and offline.  Data is read into **dataframes**, 2 dimensional tables with indexed rows and labelled columns.\n",
    "\n",
    "To use the pandas library, create an alias:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_ewU932LIO-"
   },
   "source": [
    "## From a web page\n",
    "---\n",
    "\n",
    "The code below reads all the data tables from the Wikipedia page on Glasgow.  The 8th table on the page shows population data over a period of centuries.\n",
    "\n",
    "The code reads the data from the page into a list of datatables.  The index [7] is used to access the 8th table in the list.    \n",
    "*  change the index to see other data tables  \n",
    "*  use len(datatables) to get how many tables are in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAV9q4J3t-zy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datatables = pd.read_html('https://en.wikipedia.org/wiki/Glasgow#Climate')\n",
    "df = datatables[7]  #Glasgow population data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBzjrbk8evK7"
   },
   "source": [
    "## From a local file\n",
    "---\n",
    "\n",
    "You can upload from a local file (Excel or CSV) using the code below. Visit this webpage: https://datacatalog.worldbank.org/dataset/world-bank-group-linkedin-digital-data-development/resource/75461542-d5f6-4a68-9a05#{view-graph:{graphOptions:{hooks:{processOffset:{},bindEvents:{}}}},graphOptions:{hooks:{processOffset:{},bindEvents:{}}}} and download the file public_use-talent-migration.xlsx' into the same folder as this Jupyter notebook.  After that, just use the file name to read from it.  \n",
    "\n",
    "\n",
    "*  the file has three sheets:  \"Industry Migration\", \"Country Migration\", \"Skills Migration\"  display each of the three datasets.\n",
    "*  visit this page: https://data.un.org/ and download the CSV file on International Migrants and Refugees (the file is called 'SYB63_327_202009_International Migrants and Refugees.csv'\n",
    "*  move the file into the same folder as this Jupyter notebook\n",
    "*  use pd.read_csv(filename) to read the contents of the file and display them (6543 rows, 7 columns on 22nd June 2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w29Lrv_je0nj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "industry_migration_df = pd.read_excel(\"public_use-talent-migration.xlsx\",sheet_name=\"Industry Migration\")\n",
    "display(industry_migration_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9m4HHb3Loxe"
   },
   "source": [
    "## From a csv file hosted on Github.com\n",
    "---\n",
    "\n",
    "The code below reads the data table stored in a Comma Separated Values file (this is a text file containing rows of data with each column within the row separated from the next column by a comma).  \n",
    "\n",
    "CSV files on Github must be accessed using the link to the raw data.  To get this link, find the file you want to read on Github (there are a number of csv files here: https://github.com/futureCodersSE/working-with-data/tree/main/Data%20sets).  Click on the file name, then click on Raw (top right of the file contents), then copy the link.  Change the value of url to this link and read the new file.\n",
    "\n",
    "*  read, and display, the contents of the file 'WHO POP TB all.csv' at the link above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mVk6FwZ-XZu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/Paisley-Weather-Data.csv\"\n",
    "paisley_weather_df = pd.read_csv(url)\n",
    "display(paisley_weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVBfyjCQMVvA"
   },
   "source": [
    "## From an Excel file hosted on Github.com\n",
    "---\n",
    "\n",
    "The code below reads the data table from a sheet in an Excel file.  If you don't specify a sheet then it will assume that you want to read the data from the first sheet in the Excel workbook (sheet_name = 0).  If you don't know the sheet name but know it is the second sheet, you can use sheet_name = 1, or 2 for the third sheet, etc.\n",
    "\n",
    "For an Excel file hosted on Github you still need the Raw file but you will need to right-click on the word _Raw_ in the View Raw message when you open the file, then Copy Link Address.  Then you can paste it into the code cell to set the url.\n",
    "\n",
    "*  read the file 'Income-Data.xlsx' from the Github folder: https://github.com/futureCodersSE/working-with-data/tree/main/Data%20sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Att2-4UtBAhW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://github.com/futureCodersSE/working-with-data/blob/main/Data%20sets/public_use-talent-migration.xlsx?raw=true\"\n",
    "industry_migration_df = pd.read_excel(url,sheet_name=\"Industry Migration\")\n",
    "display(industry_migration_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NQA3HFJOT8X"
   },
   "source": [
    "## From an API which delivers the data in JSON format\n",
    "---\n",
    "\n",
    "The code below requests the data from the url.  This is a bit more tricky than the other ways to get the data as how you access the data will depend on how it is organised.\n",
    "\n",
    "In this example, the data is returned as a dictionary, which will have the key 'data' against which the actual data is stored.  In the example, the data has been taken from the 'data' key/value pair and is stored in json_data. \n",
    "\n",
    "Again, in this example, the json_data is a list of json_objects but it only has one object in the list.  Try adding the line `print(json_data)` to see this.  \n",
    "\n",
    "data_table is the first object in the json_data list.  Try adding the line `print(data_table)` to see this.\n",
    "\n",
    "In this example, the data table object has three keys, 'to', 'from' and 'regions'.  The 'regions' value is the data we want to use in our dataframe, so we normalize this json data into a pandas dataframe (df), which you can see as the output.  \n",
    "\n",
    "Each API is likely to deliver its data in a different format and so you will need to be happy to read the documentation and to inspect the data to see what keys and indexes you need to access.\n",
    "\n",
    "For information on the format of the data, see https://carbon-intensity.github.io/api-definitions/#regional\n",
    "\n",
    "*  read the carbon instensity data from your postcode region (https://carbon-intensity.github.io/api-definitions/regional/postcode/XX9 (repoacing XX9 with the first part of your postcode.  You will need to replace the ['regions'] from the 8th line of code as the postcode area doesn't have region, and replace it with ['data'][0]['generationmix'] as there is specific data to look at from this postcode.  Refer to: https://carbon-intensity.github.io/api-definitions/#get-regional-postcode-postcode for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uEDgTjJFYiZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = \"https://api.carbonintensity.org.uk/regional/postcode/CM24\"\n",
    "try:\n",
    "    json_data = requests.get(url).json()['data']\n",
    "    data_table = json_data[0]\n",
    "    carbon_intensity_df = pd.json_normalize(data_table['data'][0]['generationmix'])\n",
    "    display(carbon_intensity_df)\n",
    "except:\n",
    "    print(\"Data error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGV-xKl6TUgD"
   },
   "source": [
    "## Read from SQL database\n",
    "---\n",
    "\n",
    "In this example we are using a simple database created using SQLITE, widely used with Python.  it is an SQL (Structured Query Language) database with a set of related data tables (surveys, species and plots)\n",
    "\n",
    "Create three dataframes (df_surveys, df_species and df_plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJu3qLFSUKEk"
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# connect to database taken from http://dx.doi.org/10.6084/m9.figshare.1314459\n",
    "conn = sqlite3.connect('Data/portal_mammals.sqlite')\n",
    "\n",
    "# get a list of the tables in this database\n",
    "tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "\n",
    "# add a loop to display the names of the tables (you will need to use for i in range)\n",
    "\n",
    "    \n",
    "# read the contents of the surveys data table into a dataframe df_surveys (35549 rows, 5 columns\n",
    "df_surveys = pd.read_sql_query(\"SELECT * FROM plots\",conn)\n",
    "display(df)\n",
    "\n",
    "# add code to read the contents of the species data table (53 rows, 4 columns)\n",
    "\n",
    "\n",
    "# add code to read the contents of the plots data table (23 rows, 2 columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM1Um6EMSirA"
   },
   "source": [
    "## Exercise - upload a CSV file to your own Github account and write Python code to load it into a dataframe\n",
    "---\n",
    "\n",
    "1.  Download the CSV file at this link to your downloads folder on your computer: https://drive.google.com/file/d/15vDkpCKqlRHQt8f8VHER97fIqZytIAtu/view?usp=sharing \n",
    "\n",
    "2.  Create a folder called 'Data Sets' and move the CSV file into the Data Sets folder.\n",
    "\n",
    "3.  Log in to your Github account and navigate to the repository where you are uploading all your Colab Worksheets\n",
    "\n",
    "4.  Click on the Add File button  \n",
    "![Add file to Github](https://drive.google.com/uc?id=1szQpVcLg56yPPJc6z4wvK9mCGzSNSa5q)  Select *Upload Files* and then drag the Data Sets folder onto the page (drag the folder rather than the files in it).  Once the folder has uploaded, you will need to commit the changes.  Scroll down the page to see the Commit Changes button.  Before you commit, you can add an extended description *e.g. New folder to store data sets.*\n",
    "\n",
    "5.  Click to open the Data Sets folder in your Github repository.  Then click to select the file `housing_in_london_yearly_variables.csv`.  You will need a link to the 'raw' data version of this file.  \n",
    "![Get raw data](https://drive.google.com/uc?id=1_B9_1YK35eRpXp5kN2zBZRu0m_CIBn5i)  \n",
    "Click on 'raw', you will see just the data shown in the page.  Select the URL for this page and copy it.  **This is the link you will need.**  \n",
    "\n",
    "You can now refer to the section above 'From a csv file hosted on Github.com' and create a dataframe from your newly uploaded CSV file.\n",
    "\n",
    "### Note: \n",
    "for future data set uploads you will just need to navigate to the Data Sets folder in your Github repository and click on Add File there.  You can then just upload the file and it will sit in the Data Sets folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPPqyzwWcfkb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "----\n",
    "\n",
    "## What skills have you demonstrated in completing this notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your response:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What caused you the most difficulty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your response:* "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_retrieval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
